# LFM Vision Demo

LiquidAI が公開している高効率な VLM (Vision Language Model) **「LFM2.5-VL-1.6B」** を使用した、リアルタイム Web カメラ解析デモプロジェクトです。

ブラウザからカメラ映像を送信し、AI がその内容をテキストで解説します。

## 特徴
- **リアルタイム解析**: Web カメラの映像を一定間隔でサーバーに送信し、解析結果をログとして表示します。
- **カスタマイズ可能なプロンプト**: 解析の際、AI に送る指示（プロンプト）をブラウザ上で自由に変更できます。
- **軽量・高速**: 1.6B パラメータという軽量なモデルを使用しているため、一般的な GPU 環境でもスムーズに動作します。

## セットアップ手順

### 1. 準備するもの
- **Python**: バージョン 3.10 以上がインストールされている必要があります。
- **GPU (推奨)**: NVIDIA 製の GPU が搭載されていると、解析がより高速に行われます（CUDA 環境）。

### 2. コードのダウンロード
本プロジェクトのフォルダ（`lfm-demo`）を任意の場所に配置してください。

### 3. 仮想環境の作成と有効化
プロジェクト専用の実行環境を作成します。ターミナルで `lfm-demo` フォルダに移動し、以下のコマンドを実行してください。

```bash
# 仮想環境を作成（最初の一回のみ）
python -m venv .venv

# 仮想環境を有効化 (Windows)
.venv\Scripts\activate

# (Mac/Linux の場合)
# source .venv/bin/activate
```

有効化されると、ターミナルの先頭に `(.venv)` と表示されます。

### 4. 必要なライブラリのインストール
以下のコマンドを実行して、AI の実行に必要なパッケージを一括インストールします。

```bash
pip install -r requirements.txt
```

> [!NOTE]
> インストールには数分かかる場合があります。

## サーバーの起動

セットアップが完了したら、以下のコマンドでサーバーを起動します。

```bash
python server.py
```

ターミナルに `Loading model...` と表示され、しばらく待って `Model loaded.` と出れば準備完了です。

## 使い方

1. ブラウザ（Chrome や Edge など）を開き、アドレスバーに以下を入力してアクセスします。
   ```
   http://localhost:8000
   ```
2. カメラの使用許可を求められたら「許可」を押してください。
3. 画面上の入力欄に、AI への質問（例：「この写真には何が写っていますか？」）を入力します。
4. **「開始」** ボタンを押すと、解析が始まります。
5. 解析結果が右側の「解析ログ」エリアに次々と表示されます。
6. **「停止」** ボタンを押すと、解析がストップします。

## プロジェクト構成
- `server.py`: FastAPI を使用したメインのサーバープログラム。
- `static/`: ブラウザに表示される画面（HTML/CSS/JavaScript）を格納。
- `requirements.txt`: 必要なライブラリのリスト。
- `Specs/`: プロジェクトの仕様書。

## 技術情報
- **VLM モデル**: [LiquidAI/LFM2.5-VL-1.6B](https://huggingface.co/LiquidAI/LFM2.5-VL-1.6B)
- **バックエンド**: FastAPI / Uvicorn
- **フロントエンド**: Vanilla JS / CSS / HTML5 Canvas
